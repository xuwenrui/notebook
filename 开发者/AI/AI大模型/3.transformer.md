Transformer是由Google团队在2017年提出的，具体是由Ashish Vaswani等人在论文《Attention Is All You Need》中提出的。这篇论文标志着Transformer架构的诞生，它彻底改变了自然语言处理（NLP）领域，通过引入自注意力机制，使得模型能够并行处理输入序列，大大提高了处理效率，并且在捕捉长程依赖方面表现优异。此架构已成为现代NLP模型的核心组成部分，影响了后续一系列先进模型的发展。

![[Pasted image 20240623113421.png|500]]
提示搜索引擎提示词，概率越高提示越在上面
![[Pasted image 20240623113659.png]]

![[Pasted image 20240623113747.png]]

##### token化
短单词对应一个token，长单词可能被拆成多个token
![[Pasted image 20240623113915.png]]![[Pasted image 20240623114009.png]]
![[Pasted image 20240623114109.png]]

显示省略为3为，向量化放大数据，储存更多信息
![[Pasted image 20240623114333.png]]

![[Pasted image 20240623114448.png]]

![[Pasted image 20240623114522.png]]

#####  自注意力机制
![[Pasted image 20240623114716.png]]

词之前的相关性越强，它们之间的权重越高
###### 多注意权重
不同维度权重不同，多个编码器不共享权重
#### 解码
将输入和生成内容作为输入
![[Pasted image 20240623121606.png]]
只是用前面的词作为上下文
![[Pasted image 20240623121714.png]]


![[Pasted image 20240623121844.png]]

##### 幻觉
模型基于下一个token的概率输出，跟正确性无关
![[Pasted image 20240623121934.png]]