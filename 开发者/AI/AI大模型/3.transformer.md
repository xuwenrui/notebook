Transformer是由Google团队在2017年提出的，具体是由Ashish Vaswani等人在论文《Attention Is All You Need》中提出的。这篇论文标志着Transformer架构的诞生，它彻底改变了自然语言处理（NLP）领域，通过引入自注意力机制，使得模型能够并行处理输入序列，大大提高了处理效率，并且在捕捉长程依赖方面表现优异。此架构已成为现代NLP模型的核心组成部分，影响了后续一系列先进模型的发展。

![[Pasted image 20240623113421.png|500]]
提示搜索引擎提示词，概率越高提示越在上面
![[Pasted image 20240623113659.png]]

![[Pasted image 20240623113747.png]]

##### token化
短单词对应一个token，长单词可能被拆成多个token
![[Pasted image 20240623113915.png]]![[Pasted image 20240623114009.png]]
![[Pasted image 20240623114109.png]]

显示省略为3为，向量化放大数据，储存更多信息
![[Pasted image 20240623114333.png]]

![[Pasted image 20240623114448.png]]

![[Pasted image 20240623114522.png]]

#####  自注意力机制
![[Pasted image 20240623114716.png]]