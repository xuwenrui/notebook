**K-Means** 是最经典、应用最广泛的聚类算法之一，因其**原理简单、实现高效**，常作为无监督学习的入门首选。下面为你系统梳理其核心原理、关键细节、实战技巧及适用边界（结合你的工程背景特别标注重点）👇
![[Pasted image 20260103170001.png|625]]
---

### 🧠 **核心思想：迭代优化“组内紧凑，组间分离”**

1. **初始化**：随机选择 K 个点作为初始簇中心（centroids）
2. **分配（Assignment）**：将每个样本分配给**最近的**簇中心（通常用欧氏距离）
3. **更新（Update）**：重新计算每个簇的**均值**作为新中心
4. **重复步骤 2~3**，直到中心不再显著变化（收敛）

> ✅ **本质**：最小化**组内平方和（WCSS）**  
> $\text{WCSS} = \sum_{i=1}^K \sum_{\mathbf{x} \in C_i} \|\mathbf{x} - \boldsymbol{\mu}_i\|^2$
> 
>  其中 $C_i$ 是第 $ 个簇，μiμi​ 是其均值中心。

---

### ⚙️ **关键参数与挑战**

|问题|说明|解决方案|
|---|---|---|
|**K 值选择**|最大痛点！K 过小欠拟合，过大过拟合|**肘部法则（Elbow Method）**：<br>• 绘制 WCSS vs K 曲线<br>• 选“拐点”（下降明显变缓处）<br>**轮廓系数（Silhouette Score）**：<br>• 计算每个样本的轮廓系数 s(i)∈[−1,1]s(i)∈[−1,1]<br>• 选平均 s(i)s(i) 最大时的 K|
|**初始中心敏感**|随机初始化可能导致局部最优|**K-Means++**：<br>• sklearn 默认采用！<br>• 首个中心随机，后续按**距离概率**选（远离已有中心的点更可能被选）|
|**对异常值敏感**|均值易受离群点影响|预处理：用 IQR 或孤立森林剔除异常值<br>或改用 **K-Medoids（PAM）**（用簇内实际点作中心）|
|**仅适用凸形簇**|无法识别环形、月牙形等非凸结构|预处理：用核技巧（Kernel K-Means）<br>或改用 DBSCAN/MeanShift|